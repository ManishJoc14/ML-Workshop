# -*- coding: utf-8 -*-
"""ML-Day_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gnTUlJ1uAUoaHmypfCyERsOsK71Q80UW

# **Data Preprocessing and Feature Engineering**
"""

# Download Dataset:- https://www.kaggle.com/c/titanic/data?select=train.csv

import pandas as pd

df = pd.read_csv("/content/drive/MyDrive/ML-Workshop/train.csv")

df.head()

df.tail(10)

df.info()  # Displays a concise summary of the DataFrame

df.describe()  # Generates descriptive statistics for the DataFrame

df.isnull()

df.isnull().sum()

df.shape

len(df)

"""# **Understanding Data and its Nature**"""

import seaborn as sns  # for data visualization

sns.heatmap(df.isnull())
# sns.heatmap(df.isnull(), xticklabels=False)

# find % missing of Age

sns.countplot(x="Survived", data=df)

# find survived ration

# 'hue' divide the 'Survived' record in that part
# sns.set_style('whitegrid')
sns.countplot(x="Survived", hue="Sex", data=df, palette="rainbow")

sns.distplot(df["Age"].dropna(), kde=False)
# distplot by default contains kernel density estimate (kde). So make it false

sns.countplot(x="SibSp", data=df)

sns.distplot(df["Fare"], kde=False)

# Group by Pclass and Sex, and count the number of occurrences in each group
gender_class_count = df.groupby(["Pclass", "Sex"]).size().unstack()

# .size() calculates the size of each group
# .unstack() is used to reshape the Series into a DataFrame

import matplotlib.pyplot as plt

gender_class_count.plot(kind="bar")

plt.xlabel("Passenger Class")
plt.ylabel("Number of Passengers")
plt.title("Male to Female Ratio in Each Pclass")

plt.xticks(rotation=0)

survival_counts = df["Survived"].value_counts()
survival_counts

import matplotlib.pyplot as plt

plt.pie(
    survival_counts,
    labels=["Not Survived", "Survived"],
    autopct="%1.1f%%",
    startangle=90,
)

"""# **Data Cleaning**"""

df.describe()

# Need to clean 'Age' and 'Cabin' column
# look data distribution of these 2

sns.histplot(df["Age"], kde=False)

# Checking Outlier
sns.boxplot(x=df["Age"])

mean_age = df["Age"].mean()
median_age = df["Age"].median()
mode_age = df["Age"].mode()[0]

print(f"Mean Age: {mean_age}")
print(f"Median Age: {median_age}")
print(f"Mode Age: {mode_age}")

# If no outlier, better to replace with mean
# If outlier, better to replace with median
# If distince cluster, use mode (outlier has more gap in between)

mean_age = df["Age"].mean()

df["Age"] = df["Age"].fillna(mean_age)

df.isnull().sum()

df

sns.heatmap(df.isnull())

df["Cabin"]

df.drop("Cabin", axis=1, inplace=True)
# axis=1 means frop the column, if axis=0 drop the row
# inplace=True modify the original dataframe without returning a copy of modified dataframe

df

df.isnull().sum()

sns.heatmap(df.isnull())

df.dropna(subset=["Embarked"], inplace=True)

df.isnull().sum()

df.shape

df.head()

Q1 = df["Age"].quantile(0.25)
Q3 = df["Age"].quantile(0.75)
IQR = Q3 - Q1

lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Technique to handle outliers
# 1. Remove Outliers
# 2. Floor the Outliers (Replace extreme outliers with the maximum or minimum value)
# 3. Imputation (Replace with mean, median, mode)
# If we know the bounded value and don't have outlier, use min max normalization else use z-score

# Replace with Q1 if value is below boundary and if value is above boundary replace with Q3
df["Age"] = df["Age"].apply(
    lambda x: Q1 if x < lower_bound else (Q3 if x > upper_bound else x)
)

sns.boxplot(x=df["Age"])

"""# **Label Encoding**"""

from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()

df["Sex"]

df["Sex"] = label_encoder.fit_transform(df["Sex"])

df["Sex"]

df.head()

df["Embarked"].unique()

df.drop(["PassengerId", "Name", "Ticket"], axis=1, inplace=True)

"""# **Use One Hot Enconding**"""

# df['Embarked'] = label_encoder.fit_transform(df['Embarked'])
# df['Embarked']

df_encoded = pd.get_dummies(df, columns=["Embarked"]).astype(int)
df_encoded

df

df_encoded.head()

# Normalize the age using minmax algorithm
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()

df["Age"]

df["Age"] = scaler.fit_transform(df[["Age"]])

df["Age"]

df["Fare"] = scaler.fit_transform(df[["Fare"]])

df.head()

correlation_matrix = df_encoded.corr()

sns.heatmap(correlation_matrix, annot=True)

"""# **Building a Logisti Regression Model**"""

# Split the data into dependent and independent features
independent_data = df_encoded.drop("Survived", axis=1)
dependent_data = df_encoded["Survived"]

independent_data

"""# **Split the data into train and test**"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    independent_data, dependent_data, test_size=0.2, random_state=101
)
# random_state control the randomness of the data splitting process
# get same data for training and testing every time we run the code

X_train.shape

"""# **Performing Prediction LogisticRegression**"""

from sklearn.linear_model import LogisticRegression

logmodel = LogisticRegression()
logmodel.fit(X_train, y_train)

predicitions = logmodel.predict(X_test)

predicitions

print(y_test.tolist())

"""# **Analysing in confusion matrix**"""

from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_test, predicitions)
cm

# plt.figure(figsize=(8, 6))
sns.heatmap(
    cm,
    annot=True,
    xticklabels=["Not Survived", "Survived"],
    yticklabels=["Not Survived", "Survived"],
)

plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.title("Confusion Matrix")

# [TP, FP
#  FN, TN]

"""# **Calculating Model's Accuracy**"""

from sklearn.metrics import accuracy_score, classification_report

accuracy = accuracy_score(y_test, predicitions)
accuracy

"""# **Display which feature has more contribution to the output**"""

coefficients = logmodel.coef_[0]
coefficients

coef_df = pd.DataFrame({"Feature": X_train.columns, "Coefficient": coefficients})
coef_df

coef_df["Abs_Coefficient"] = coef_df["Coefficient"].abs()
# (ignoring the sign, since it indicates the direction of the relationship)

coef_df

"""# **This is Supervised or Unsupervised?**

# **Why not linear regression?**
"""

# For More:- https://github.com/rsharankumar/Learn_Data_Science_in_100Days/blob/master/Day41-43%20-%20EDA%20and%20Feature%20Engineering%20using%20Titanic%20Dataset/Day41-43.ipynb

"""# **Building a Random Forest Model**"""

from sklearn.ensemble import RandomForestClassifier

rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

y_pred = rf_model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
accuracy

print(classification_report(y_test, y_pred))

# weighted_precision = (0.79*107) + (0.79*71) / (107+71)

feature_importances = rf_model.feature_importances_
feature_importances

pd.DataFrame({"Feature": X_train.columns, "Coefficient": feature_importances})

# Explain the classification report .................
# Understand each feature in titanic dataset ............
